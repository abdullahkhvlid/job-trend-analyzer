# Job Trend Analyzer

Job Trend Analyzer is a real-time, scriptable job market analysis tool built with Python and Streamlit. It automates the extraction of job listings from platforms like LinkedIn and RemoteOK, processes the data, and presents interactive visual insights to help you monitor hiring trends, top skills, job locations, and company activity.

Key Features
Real-Time Scraping
Collects job listings from online platforms using custom scraping logic.

Supports manual and automatic data refresh options.

Uses threading for background scraping to keep the UI responsive.

Interactive Dashboard
Summary metrics: total job listings, unique companies, locations, and time range.

Visual breakdowns: top job titles, frequent skills, hiring by location.

Trend graphs: time-series plots of job postings over time.

Company analytics: top hiring organizations and job counts.

Filtering and Export
Filter jobs by platform, posting date, and other criteria.

Export filtered results as CSV for external analysis.

Upload custom CSV data for offline datasets.

System Design
Built with modular Python scripts for scraping and UI.

Caching support for faster dashboard loading.

Designed for easy deployment in Streamlit or cloud environments.

Requirements
Python ≥ 3.8

pip

Internet access for scraping job data

Web browser (for viewing the dashboard)

Setup and Installation
Clone the repository:

bash
Copy
Edit
git clone https://github.com/abdullahkhvlid/job-trend-analyzer.git
cd job-trend-analyzer
Create a virtual environment (optional but recommended):

bash
Copy
Edit
python -m venv venv
source venv/bin/activate        # On Windows: venv\Scripts\activate
Install dependencies:

bash
Copy
Edit
pip install -r requirements.txt
Running the Dashboard
Ensure that job_scraper.py is in the project root and generates a CSV file (linkedin_remoteok.csv) with the required structure.

To launch the Streamlit dashboard:

bash
Copy
Edit
streamlit run job_dashboard.py
Once running, open your browser and go to:

arduino
Copy
Edit
http://localhost:8501
Expected CSV Format
Your job data CSV file should follow this structure:

Column	Description
title	Job title
company	Hiring company
location	Job location
source	Platform source (e.g. LinkedIn)
skills	Comma-separated list of skills
date_posted	Date when the job was posted (YYYY-MM-DD)

Example:

nginx
Copy
Edit
Software Engineer,Tech Corp,San Francisco,LinkedIn,"Python,JavaScript",2024-01-15
File Structure
bash
Copy
Edit
job-trend-analyzer/
├── job_dashboard.py         # Streamlit-based frontend
├── job_scraper.py           # Data scraper for job listings
├── requirements.txt         # Dependency list
├── linkedin_remoteok.csv    # Scraped job data (generated)
└── README.md                # Documentation
Configuration Notes
Auto-refresh options are controlled from the sidebar in the Streamlit UI.

CSV input can be uploaded manually via the dashboard or auto-generated by the scraper.

Scraper runs in a separate thread to ensure dashboard responsiveness.

Known Issues & Troubleshooting
Scraper not working: Confirm selectors are up-to-date and target websites haven’t changed their layout.

Data not loading: Ensure the expected CSV file (linkedin_remoteok.csv) exists and matches schema.

UI unresponsive: Try increasing refresh intervals or disabling auto-refresh if your system is under heavy load.

Permission errors: Run the app with appropriate file system permissions.

Contributions
If you'd like to improve the dashboard, optimize scraping logic, or contribute new features:

Fork the repository

Create a branch: git checkout -b feature-name

Make your changes

Commit and push: git commit -m "Brief description" + git push

Open a pull request with a clear explanation
